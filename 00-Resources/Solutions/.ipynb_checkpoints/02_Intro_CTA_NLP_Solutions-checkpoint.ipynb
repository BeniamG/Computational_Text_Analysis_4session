{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Natural Language Processing (NLP) using Python's NLTK\n",
    "\n",
    "One of the most frequent tasks in computational text analysis is quickly summarizing the content of text. In this lesson we will learn two ways of summarizing text using Python's nltk, and in the process learn some quick and easy NLP techniques.\n",
    "\n",
    "Natural Language Processing is an umbrella term that incorporates many techniques and methods to process, analyze, and understand natural languages (as opposed to artificial languages like logics, or Python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Overview\n",
    "\n",
    "#### Learning Goals:\n",
    "The goal of this lesson is to jump right into text analysis and natural language processing. Rather than starting with the nitty gritty of programming in Python, this lesson will demonstrate some neat things you can do with a minimal amount of coding and will give you an understanding of why you may want to learn the nitty gritty. An additional goal of this lesson is to start to get you thinking about analyzing texts via computational methods.  <br /> <br />  By the end of the lesson you will learn how to quickly summarize a text via counting the most frequent words, nouns, and verbs. More specifically, you will:\n",
    "\n",
    "* Gain an intuition about how computers process text, and how this is different than how humans read it\n",
    "* Learn some of the basic functions in the nltk package, such as tokenizing texts and part-of-speech tagging, and learn why these might help researchers analyze text\n",
    "* Get started with some basic coding (although don't worry if you don't understand everything)\n",
    "\n",
    "\n",
    "#### Lesson Outline:\n",
    "1. Assigning Text as Variables in Python\n",
    "    * Exercise 1.1\n",
    "* Tokenizing Text and Counting Words\n",
    "    * Exercise 2.1\n",
    "* Pre-Processing: \n",
    "    * Changing words to lowercase\n",
    "    * Removing stop words\n",
    "    * Removing punctuation\n",
    "    * Exercise 3.1\n",
    "* Part-of-Speech Tagging\n",
    "    * Exercise 4.1\n",
    "* Illustration: Guess the Unknown Text\n",
    "    * Exercise 5.1\n",
    "* Word Frequency Plot\n",
    "    * Exercise 6.1\n",
    "* Part-of-Speech Tagging\n",
    "    * Tagging tokens\n",
    "    * Counting tagged tokens\n",
    "    * Exercise 7.1\n",
    "* If there's time: concordances\n",
    "\n",
    "\n",
    "#### Key Jargon:\n",
    "* *coding or programming*: \n",
    "    * The purpose of programming is to find a sequence of instructions that will enable a computer to perform a specific task or solve a given problem. It involves writing those instructions in a specific *programming language*, in our case, Python.\n",
    "* *script*:\n",
    "    * A block of executable code, typically saved in a executable file. For example, script1.py\n",
    "* *packages and modules*: \n",
    "    * Python files, or collections of files, that implement a set of pre-made functions (so we don't have to write all of the functions ourselves). To utilize a module we use the import function.\n",
    "* *parse*: \n",
    "    * the process of analysing a string of symbols, in this case the symbols that make up natural language. This can also include understanding, or parsing, computer code.\n",
    "* *variable*: \n",
    "    * A variable is something that holds a value that may change. In simplest terms, a variable is just a box that you can put stuff in. You can use variables to store all kinds of stuff, including numbers and letters.\n",
    "* *assigning a variable*: \n",
    "    * telling Python what you want to name the variable, and what is stored in the variable.\n",
    "* *string*: \n",
    "    * a type of variable the consists of a sequence of characters in a particular order. Characters can be anything, including letters or numbers. The order of a string is fixed.\n",
    "* *list*: \n",
    "    * a type of variable that consists of a sequence of elements. The order is fixed.\n",
    "* *stop words*: \n",
    "    * the most common words in a language.\n",
    "\n",
    "#### Further Resources:\n",
    "\n",
    "Check out the full range of techniques included in Python's nltk package here: http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Assigning Text as a Variable in Python\n",
    "\n",
    "First, we assign a sample sentence, our \"text\", to a variable called \"sentence\" (the name of the variable is arbitrary). Printing the sentence shows what the variable \"sentence\" contains. \n",
    "\n",
    "Note: This sentence is a quote about what digital humanities means, from digital humanist Kathleen Fitzpatrick. Source: \"On Scholarly Communication and the Digital Humanities: An Interview with Kathleen Fitzpatrick\", *In the Library with the Lead Pipe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Anything on a line starting with a hashtag is called a comment, and is meant to clarify code for human readers.\n",
    "#The computer ignores these lines.\n",
    "\n",
    "#assign the desired sentence to the variable called 'sentence.' This variable type is called a string.\n",
    "sentence = \"For me it has to do with the work that gets done at the crossroads of digital media and traditional humanistic study. And that happens in two different ways. On the one hand, it’s bringing the tools and techniques of digital media to bear on traditional humanistic questions; on the other, it’s also bringing humanistic modes of inquiry to bear on digital media.\"\n",
    "\n",
    "#print the contents of the variable 'sentence'\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First import the Python package nltk (Natural Language Tool Kit)\n",
    "import nltk\n",
    "\n",
    "#import the function to split the text into separate words from the NLTK package\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#create new variable that applies the word_tokenize function to our sentence.\n",
    "sentence_tokens = word_tokenize(sentence)\n",
    "\n",
    "#This new variable contains the tokenized text, and is now a variable type called a list.\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.1\n",
    "Use the  space below to complete the following tasks:\n",
    "1. Create a new variable called `test_sentence` and assign the following content to `test_sentence`: \"The itsy bitsy spider climbed up the waterspout. Down came the rain; and washed the spider out. Out came the sun; and dried up all the rain; and the itsy bitsy spider climbed up the spout again.\"\n",
    "* Print the variable `test_sentence`\n",
    "* Create a new variable called `test_sentence_tokens` that applies the `word_tokenize` function to `test_sentence`\n",
    "* Print the variable `test_sentence_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a comment.  It's a good idea to use comments to describe what you are doing with your code. \n",
    "# Enter a comment for each of the steps in exercise 0.1, and then enter the solution below it.\n",
    "\n",
    "# 1. creating test sentence variable\n",
    "test_sentence = \"The itsy bitsy spider climbed up the waterspout. Down came the rain; and washed the spider out. Out came the sun; and dried up all the rain; and the itsy bitsy spider climbed up the spout again.\"\n",
    "\n",
    "# 2. Printing the sentence\n",
    "print(test_sentence)\n",
    "\n",
    "# 3. Create test_sentence_tokens\n",
    "test_sentence_tokens = word_tokenize(test_sentence)\n",
    "\n",
    "# 4. Print test_sentence_tokens\n",
    "print(test_sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. Tokenizing Text and Counting Words\n",
    "\n",
    "The above output is how a human would read that sentence. Next we look at different ways in which a computer \"reads\", or *parses*, that sentence, and some simple ways to analyze/summarize it.\n",
    "\n",
    "Often the first step needed to enable a computer to parse text is changing the sentence into \"tokens.\" This is referred to as *tokenizing* text. Each token roughly corresponds to either words or punctuation. In essence, this process divides the sentence into little bits that the computer can process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice each token is either a word or punctuation. [Note: in the coming days we will see other methods to tokenize text. While seemingly simple, tokenizing text is not a trivial task.]\n",
    "\n",
    "Why is this helpful?\n",
    "\n",
    "We can now summarize the sentence/text in interesting and potentially helpful ways. For example, we can count the number of tokens in the sentence, which roughly corresponds to the number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The number of tokens is the length of the list, or the number of elements in the list\n",
    "print(len(sentence_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also count the most frequent words, which can help us quickly summarize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#apply the nltk function FreqDist to count the number of times each token occurs.\n",
    "word_frequency = nltk.FreqDist(sentence_tokens)\n",
    "\n",
    "#print out the 10 most frequent words using the function most_common\n",
    "print(word_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent words do suggest what the sentence is about, in particular the words \"humanistic\", \"digital\", \"media\", and \"traditional\".\n",
    "\n",
    "But there are many frequent words that are not helpful in summarizing the text, for example, \"the\", \"and\", \"to\", and \".\" So the most frequent words do not necessarily help us understand the content of a text.\n",
    "\n",
    "How can we use a computer to identify important, interesting, or content words in a text? There are many ways to do this, a few of which we'll cover in this workshop. Today, we'll look at two simple ways to identify words that will help us summarize the content of a text. We'll see additional ways of doing this in future workshops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1\n",
    "\n",
    "In the space provided below, write a comment describing each step of your process, then write code to answer the following questions and execute the following tasks:\n",
    "1. How many tokens appear in the sentence we used in exercise 1.1 above?\n",
    "* Create a new variable that counts the number of times each token occurs.\n",
    "* Print the five most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How many tokens appear in the sentence we used in exercise 1.1 above?\n",
    "print(len(test_sentence_tokens))\n",
    "\n",
    "# Create a new variable that counts the number of times each token occurs.\n",
    "test_word_frequency = nltk.FreqDist(test_sentence_tokens)\n",
    "\n",
    "# Print the five most common words\n",
    "print(test_word_frequency.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pre-Processing: Lower Case, Removing Stop Words and Punctuation\n",
    "\n",
    "First, scholars typically go through a number of pre-processing steps before getting to the actual analysis. One of these step is converting all words to lower-case, so that the word \"Humanities\" and \"humanities\" count as the same word. (For some tasks this is appropriate. Think of reasons why we might NOT want to do this.)\n",
    "\n",
    "To convert to lower case we use the function lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_tokens_lc = [word.lower() for word in sentence_tokens]\n",
    "\n",
    "#see the result\n",
    "print(sentence_tokens_lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words like \"the\", \"to\", and \"and\" are what text analysis call \"stop words.\" Stop words are the most common words in a language, and while necessary and useful for some analysis purposes, do not tell us much about the *substance* of a text. Another common pre-processing steps is to simply remove punctuation and stop words. NLTK contains a built-in stop words list, which we use to remove stop words from our list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import the stopwords list\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#take a look at what stop words are included:\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a new variable that contains the sentence tokens without the stopwords\n",
    "sentence_tokens_clean = [word for word in sentence_tokens_lc if word not in stopwords.words('english')]\n",
    "\n",
    "#see what words we're left with\n",
    "print(sentence_tokens_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation also does not help us understand the substance of a text, so we'll remove punctuation in a similar fashion. [Again, think about tasks where me may not want to remove punctuation.] There are many many ways to do this. For now, we'll create a list of punctuation tokens, similar to the list of stop words, and remove them from our list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a list of punctuation symbols\n",
    "#there are better ways to do this which we'll get to later, but we'll keep it simple here\n",
    "punctuation = [\".\", \";\", \",\", \"'\",\"’\", '\"', \"!\"]\n",
    "sentence_tokens_clean = [word for word in sentence_tokens_clean if word not in punctuation]\n",
    "\n",
    "#see what's left\n",
    "print(sentence_tokens_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after our pre-processing steps, let's re-count the most frequent words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_frequency_clean = nltk.FreqDist(sentence_tokens_clean)\n",
    "print(word_frequency_clean.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better! The 10 most frequent words now give us a pretty good sense of the substance of this sentence. But we still have problems. For example, the word \"it's\" sneaked in there. One solution is to keep adding stop words to our stop word list, but this could go on forever and is not a good solution when processing lots of text.  There's another way of identifying content words, and it involves identifying the part of speech of each word, which we'll cover in the next section.\n",
    "\n",
    "Before we turn to that, let's look at another set of helpful operations which help us to understand how common words are.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.1\n",
    "For this exercise we'll undertake a few manipulations to help you familiarize yourself with the code above.\n",
    "1. Create a version of the sentence from exercise 1.1 that is free from stopwords.\n",
    "* Create a version of the sentence from exercise 1.1 that is free from punctuation.\n",
    "* Print the tokenized sentence from exercise 1.1 without stopwords or punctuation.\n",
    "* Print the most frequent words from the example in 1.1 after removing stopwords and punctuation.  \n",
    "* EXTRA CREDIT: Can you create and print a variable with the sentence from this section into all *UPPERCASE*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#FIRST CONVERT TO LOWERCASE!!!\n",
    "test_sentence_tokens_lc = [word.lower() for word in test_sentence_tokens]\n",
    "\n",
    "# Create a version of the sentence from exercise 1.1 that is free from stopwords.\n",
    "test_sentence_tokens_clean = [word for word in test_sentence_tokens_lc if word not in stopwords.words('english')]\n",
    "\n",
    "# Create a version of the sentence from exercise 1.1 that is free from punctuation.\n",
    "test_sentence_tokens_clean = [word for word in test_sentence_tokens_clean if word not in punctuation]\n",
    "\n",
    "# Print the tokenized sentence from exercise 1.1 without stopwords or punctuation.\n",
    "print(test_sentence_tokens_clean)\n",
    "\n",
    "# Print the most frequent words from the example in 1.1 after removing stopwords and punctuation.\n",
    "test_word_frequency_clean = nltk.FreqDist(test_sentence_tokens_clean)\n",
    "print(test_word_frequency_clean.most_common(10))\n",
    "\n",
    "# EXTRA CREDIT: Can you create and print a variable with the sentence from this section into all UPPERCASE?\n",
    "test_sentence_tokens_uc = [word.upper() for word in test_sentence_tokens]\n",
    "print(test_sentence_tokens_uc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Part-of-Speech Tagging\n",
    "\n",
    "You may have noticed that stop words are typically short function words. Intuitively, if we could identify the part of speech of a word, we would have another way of identifying words of substance. NLTK can do that too!\n",
    "\n",
    "NLTK has a function that will tag the part of speech of every token in a text. For this, we go back to our original tokenized text, with the stop words and punctuation.\n",
    "\n",
    "NLTK uses the Penn Treebank Project to tag the part-of-speech of the words. You can find a list of all the part-of-speech tags here:\n",
    "\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use the nltk pos function to tag the tokens\n",
    "tagged_sentence_tokens = nltk.pos_tag(sentence_tokens)\n",
    "\n",
    "#view new variable\n",
    "print(tagged_sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes more complicated code. Stay with me, but focus more on the output and understanding ways in which you as a researcher can use the output, rather than understanding every line of code.\n",
    "\n",
    "We can count the part-of-speech tags in a similar way we counted words, to output the most frequent types of words in our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_frequency = nltk.FreqDist(tag for (word, tag) in tagged_sentence_tokens)\n",
    "tagged_frequency.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sentence contains a lot of adjectives. So let's first look at the most frequent adjectives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adjectives = [word for word,pos in tagged_sentence_tokens if pos == 'JJ' or pos=='JJR' or pos=='JJS']\n",
    "\n",
    "#print all of the adjectives\n",
    "print(adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#calculate the frequency of the adjectives\n",
    "freq_adjectives=nltk.FreqDist(adjectives)\n",
    "\n",
    "#print the most frequent adjectives\n",
    "print(freq_adjectives.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verbs = [word for word,pos in tagged_sentence_tokens if pos == 'VB' or pos=='VBD' or pos=='VBG' or pos=='VBN' or pos=='VBP' or pos=='VBZ']\n",
    "\n",
    "#print all of the verbs\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#calculate the frequency of the verbs\n",
    "freq_verbs=nltk.FreqDist(verbs)\n",
    "\n",
    "#print the most frequent verbs\n",
    "print(freq_verbs.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we bring all of this together we get a pretty good summary of the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(freq_adjectives.most_common(3))\n",
    "print(freq_verbs.most_common(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.1\n",
    "In the section above we tagged all the adjectives and the verbs, and we then compared the most frequent uses of each.  For this exercise we'll do the same for nouns.  The *pos* codes for nouns are `NN` and `NNS`.\n",
    "1. Create a variable called `nouns` which contains all the tokens which are tagged `NN` or `NNS`\n",
    "* Print the variable `nouns`\n",
    "* Calculate the frequency of the `nouns`\n",
    "* print the most frequent `nouns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a variable called nouns which contains all the tokens which are tagged \"NN\" or \"NNS\"\n",
    "nouns = [word for word,pos in tagged_sentence_tokens if pos == 'NN' or pos=='NNS']\n",
    "\n",
    "# Print the variable nouns\n",
    "print(nouns)\n",
    "\n",
    "# Calculate the frequency of the nouns\n",
    "freq_nouns = nltk.FreqDist(nouns)\n",
    "\n",
    "# print the most frequent nouns\n",
    "print(freq_nouns.most_common(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Illustration: Guess the Unknown Text\n",
    "\n",
    "To illustrate this process on a slightly larger scale, we will do the exactly what we did above, but will do so on four unknown texts. Your challenge: guess the text from the most frequent words, nouns, and verbs. We will do this in one chunk of code, so another challenge for you during breaks or the next few weeks is to see how much of the following code you can follow (or, in computer science terms, how much of the code you can parse). If the answer is none, not to worry! The more you interact with other people's code, the more you'll be able to decide what is worth learning and what is worth *borrowing*.\n",
    "\n",
    "Note: this codes requires one thing we haven't covered: reading a .txt file from your hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import the package 'string' for a different way of removing punctuation. It's simply a more complete list of punctuation than we created above.\n",
    "import string\n",
    "punctuations = list(string.punctuation)\n",
    "\n",
    "#see what punctuation is included\n",
    "print(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read the text files from your hard drive, assign first unknown text to variable 'text1', the second text to variable 'text2', etc.\n",
    "text1 = open('../../A-Data/text1.txt', encoding='utf-8').read()\n",
    "text2 = open('../../A-Data/text2.txt', encoding='utf-8').read()\n",
    "text3 = open('../../A-Data/text3.txt', encoding='utf-8').read()\n",
    "text4 = open('../../A-Data/text4.txt', encoding='utf-8').read()\n",
    "\n",
    "###word frequencies\n",
    "\n",
    "#tokenize texts\n",
    "text1_tokens = word_tokenize(text1)\n",
    "text2_tokens = word_tokenize(text2)\n",
    "text3_tokens = word_tokenize(text3)\n",
    "text4_tokens = word_tokenize(text4)\n",
    "\n",
    "#pre-process for word frequency\n",
    "#lowercase\n",
    "text1_tokens_lc = [word.lower() for word in text1_tokens]\n",
    "text2_tokens_lc = [word.lower() for word in text2_tokens]\n",
    "text3_tokens_lc = [word.lower() for word in text3_tokens]\n",
    "text4_tokens_lc = [word.lower() for word in text4_tokens]\n",
    "\n",
    "#remove stopwords\n",
    "text1_tokens_clean = [word for word in text1_tokens_lc if word not in stopwords.words('english')]\n",
    "text2_tokens_clean = [word for word in text2_tokens_lc if word not in stopwords.words('english')]\n",
    "text3_tokens_clean = [word for word in text3_tokens_lc if word not in stopwords.words('english')]\n",
    "text4_tokens_clean = [word for word in text4_tokens_lc if word not in stopwords.words('english')]\n",
    "\n",
    "#remove punctuation using the list of punctuation from the string package\n",
    "text1_tokens_clean = [word for word in text1_tokens_clean if word not in punctuations]\n",
    "text2_tokens_clean = [word for word in text2_tokens_clean if word not in punctuations]\n",
    "text3_tokens_clean = [word for word in text3_tokens_clean if word not in punctuations]\n",
    "text4_tokens_clean = [word for word in text4_tokens_clean if word not in punctuations]\n",
    "\n",
    "#frequency distribution\n",
    "text1_word_frequency = nltk.FreqDist(text1_tokens_clean)\n",
    "text2_word_frequency = nltk.FreqDist(text2_tokens_clean)\n",
    "text3_word_frequency = nltk.FreqDist(text3_tokens_clean)\n",
    "text4_word_frequency = nltk.FreqDist(text4_tokens_clean)\n",
    "\n",
    "###part-of-speech frequencies\n",
    "\n",
    "#tag part-of-speech\n",
    "text1_tagged = nltk.pos_tag(text1_tokens)\n",
    "text2_tagged = nltk.pos_tag(text2_tokens)\n",
    "text3_tagged = nltk.pos_tag(text3_tokens)\n",
    "text4_tagged = nltk.pos_tag(text4_tokens)\n",
    "\n",
    "#most frequent nouns and verbs\n",
    "text1_nouns = [word for word,pos in text1_tagged if pos=='NN' or pos=='NNS']\n",
    "text2_nouns = [word for word,pos in text2_tagged if pos=='NN' or pos=='NNS']\n",
    "text3_nouns = [word for word,pos in text3_tagged if pos=='NN' or pos=='NNS']\n",
    "text4_nouns = [word for word,pos in text4_tagged if pos=='NN' or pos=='NNS']\n",
    "text1_freq_nouns=nltk.FreqDist(text1_nouns)\n",
    "text2_freq_nouns=nltk.FreqDist(text2_nouns)\n",
    "text3_freq_nouns=nltk.FreqDist(text3_nouns)\n",
    "text4_freq_nouns=nltk.FreqDist(text4_nouns)\n",
    "\n",
    "text1_verbs = [word for word,pos in text1_tagged if pos == 'VB' or pos=='VBD' or pos=='VBG' or pos=='VBN' or pos=='VBP' or pos=='VBZ']\n",
    "text2_verbs = [word for word,pos in text2_tagged if pos == 'VB' or pos=='VBD' or pos=='VBG' or pos=='VBN' or pos=='VBP' or pos=='VBZ']\n",
    "text3_verbs = [word for word,pos in text3_tagged if pos == 'VB' or pos=='VBD' or pos=='VBG' or pos=='VBN' or pos=='VBP' or pos=='VBZ']\n",
    "text4_verbs = [word for word,pos in text4_tagged if pos == 'VB' or pos=='VBD' or pos=='VBG' or pos=='VBN' or pos=='VBP' or pos=='VBZ']\n",
    "\n",
    "text1_freq_verbs=nltk.FreqDist(text1_verbs)\n",
    "text2_freq_verbs=nltk.FreqDist(text2_verbs)\n",
    "text3_freq_verbs=nltk.FreqDist(text3_verbs)\n",
    "text4_freq_verbs=nltk.FreqDist(text4_verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the variables are assigned as desired. We can now print out the most frequent words, nouns, and verbs for each text. Again, don't worry if you don't understand all the code here. [Note: many of these print statements are so humans can better read the output.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Frequent words for Text1:\")\n",
    "print(\"_________________________\")\n",
    "\n",
    "for word in text1_word_frequency.most_common(20):\n",
    "    print(word[0])\n",
    "print()\n",
    "print(\"Frequent nouns for Text1\")\n",
    "print(\"________________________\")\n",
    "for word in text1_freq_nouns.most_common(20):\n",
    "    print(word[0])\n",
    "print()\n",
    "print(\"Frequent verbs for Text1\")\n",
    "print(\"________________________\")\n",
    "for word in text1_freq_verbs.most_common(20):\n",
    "    print(word[0])\n",
    "    \n",
    "print()\n",
    "print(\"------------------------\")\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"------------------------\")\n",
    "print()\n",
    "\n",
    "print(\"Frequent words for Text2\")\n",
    "print(\"________________________\")\n",
    "for word in text2_word_frequency.most_common(20):\n",
    "    print(word[0])\n",
    "\n",
    "print()\n",
    "print(\"Frequent nouns for Text2\")\n",
    "print(\"________________________\")\n",
    "for word in text2_freq_nouns.most_common(20):\n",
    "    print(word[0])\n",
    "\n",
    "print()\n",
    "print(\"Frequent verbs for Text2\")\n",
    "print(\"________________________\")\n",
    "for word in text2_freq_verbs.most_common(20):\n",
    "    print(word[0])\n",
    "    \n",
    "print()\n",
    "print(\"------------------------\")\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"------------------------\")\n",
    "print()\n",
    "\n",
    "print(\"Frequent words for Text3:\")\n",
    "print(\"_________________________\")\n",
    "\n",
    "for word in text3_word_frequency.most_common(20):\n",
    "    print(word[0])\n",
    "print()\n",
    "print(\"Frequent nouns for Text3\")\n",
    "print(\"________________________\")\n",
    "for word in text3_freq_nouns.most_common(20):\n",
    "    print(word[0])\n",
    "print()\n",
    "print(\"Frequent verbs for Text3\")\n",
    "print(\"________________________\")\n",
    "for word in text3_freq_verbs.most_common(20):\n",
    "    print(word[0])\n",
    "    \n",
    "print()\n",
    "print(\"------------------------\")\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"------------------------\")\n",
    "print()\n",
    "\n",
    "print(\"Frequent words for Text4:\")\n",
    "print(\"_________________________\")\n",
    "\n",
    "for word in text4_word_frequency.most_common(20):\n",
    "    print(word[0])\n",
    "print()\n",
    "print(\"Frequent nouns for Text4\")\n",
    "print(\"________________________\")\n",
    "for word in text4_freq_nouns.most_common(20):\n",
    "    print(word[0])\n",
    "print()\n",
    "print(\"Frequent verbs for Text4\")\n",
    "print(\"________________________\")\n",
    "for word in text4_freq_verbs.most_common(20):\n",
    "    print(word[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5.1\n",
    "\n",
    "No code here, just a few questions to think about:\n",
    "\n",
    "1. Can you guess the text?\n",
    "* What further things can we learn from these lists? In particular, what can we learn from comparing these four texts?\n",
    "* What next steps would you want to take if you were to further compare these novels?\n",
    "\n",
    "We can do many more things with the Python package NLTK. Pending time, here are a few other tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Word Frequency Plot\n",
    "\n",
    "In the section above we have succeeded in using the skills we've learned to compare commonly used words across texts.  That can surface some of the unique elements within texts.  To take this one step further, we can also depict these differences with graphs.  \n",
    "\n",
    "In the code below we will create a representation of the most frequent words which appear in our first text *Moby Dick*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text1_word_frequency.plot(50, cumulative=True)\n",
    "text1_word_frequency.plot(50, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In different ways, these two graphs demonstrate a key feature of most text: they are made up of a few very common words, and a few very infrequent words.  Much of what we do in computational text analysis is try to isolate the middle of the distribution by eliminating those words which are most and least frequent in a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.1\n",
    "In the space below, write code that will produce similar graphs for the other three texts. Remember to comment you code as you compose it.  Are the curves relatively similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Solution for text2\n",
    "text2_word_frequency.plot(50, cumulative=True)\n",
    "text2_word_frequency.plot(50, cumulative=False)\n",
    "\n",
    "# Solution for text3\n",
    "text3_word_frequency.plot(50, cumulative=True)\n",
    "text3_word_frequency.plot(50, cumulative=False)\n",
    "\n",
    "# Solution for text4\n",
    "text4_word_frequency.plot(50, cumulative=True)\n",
    "text4_word_frequency.plot(50, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Concordances and Similar Words using NLTK\n",
    "\n",
    "Maybe we don't just want to know frequent words, but we want to know the way specific words are used. Concordances show us every occurrence of a given word, together with some context. This, combined with a function that should which words are used in a similar context as a given word, can help us understand the way in which a word is used in a text. \n",
    "\n",
    "To illustrate this, we can compare the way the word \"people\" is used in our four texts. To use some nltk functions on text, for example concordances, we need to first transform the text into an NLTK text object. This will be useful for using these functions on your own text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#first tokenize the two texts\n",
    "text1_tokens = word_tokenize(text1)\n",
    "text2_tokens = word_tokenize(text2)\n",
    "text3_tokens = word_tokenize(text3)\n",
    "text4_tokens = word_tokenize(text4)\n",
    "\n",
    "#then transform the tokenized text into an NLTK text object\n",
    "text1_nltk = nltk.Text(text1_tokens)\n",
    "text2_nltk = nltk.Text(text2_tokens)\n",
    "text3_nltk = nltk.Text(text3_tokens)\n",
    "text4_nltk = nltk.Text(text4_tokens)\n",
    "\n",
    "#the variables text1_nltk and text2_nltk are now nltk text objects:\n",
    "print(text1_nltk)\n",
    "print(text2_nltk)\n",
    "print(text3_nltk)\n",
    "print(text4_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now we can use the concordance function to display the word in its context\n",
    "text1_nltk.concordance(\"people\")\n",
    "print()\n",
    "text2_nltk.concordance(\"people\")\n",
    "print()\n",
    "text3_nltk.concordance(\"people\")\n",
    "print()\n",
    "text4_nltk.concordance(\"people\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk function *similar* prints out words that are used in the same context as *\"people\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Melville\")\n",
    "text1_nltk.similar(\"people\")\n",
    "print()\n",
    "print(\"Austen\")\n",
    "text2_nltk.similar(\"people\")\n",
    "print()\n",
    "print(\"Marx\")\n",
    "text3_nltk.similar(\"people\")\n",
    "print()\n",
    "print(\"Machiavelli\")\n",
    "text4_nltk.similar(\"people\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we learn from this? We see a wide array of different context for this word which evokes different objectives frome each of the authors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7.1\n",
    "In the space below, pick another word which you think might be interesting to compare across these texts. This is an interesting opportunity to start thinking about how best to utilize the methods we've gone over today.  \n",
    "1. Use the word you've chosen to print the concordances of that word.  \n",
    "* Using nltk's \"similar\" function, print the words used in the context of your chosen word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the word you've chosen to print the concordances of that word.\n",
    "print(\"Melville\")\n",
    "text1_nltk.concordance(\"new\")\n",
    "print()\n",
    "print(\"Austen\")\n",
    "text2_nltk.concordance(\"new\")\n",
    "print()\n",
    "print(\"Marx\")\n",
    "text3_nltk.concordance(\"new\")\n",
    "print()\n",
    "print(\"Machiavelli\")\n",
    "text4_nltk.concordance(\"new\")\n",
    "print()\n",
    "\n",
    "# Using nltk's \"similar\" function, print the words used in the context of your chosen word.\n",
    "print(\"Melville\")\n",
    "text1_nltk.similar(\"new\")\n",
    "print()\n",
    "print(\"Austen\")\n",
    "text2_nltk.similar(\"new\")\n",
    "print()\n",
    "print(\"Marx\")\n",
    "text3_nltk.similar(\"new\")\n",
    "print()\n",
    "print(\"Machiavelli\")\n",
    "text4_nltk.similar(\"new\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
