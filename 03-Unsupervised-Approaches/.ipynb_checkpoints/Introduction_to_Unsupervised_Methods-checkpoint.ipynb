{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Unsupervised Approaches using scikit-learn\n",
    "\n",
    "There are two libraries that dominate text analysis in Python. The first is NLTK, which implements a range of natural language processing techniques. You learned about this in part 2 of this series yesterday.\n",
    "\n",
    "The other dominant library is scikit-learn, which, at its most basic, provides a function to create a memory-efficient document-term matrix. It also implements a variety of quite sophisticated machine learning techniques that you can use on your text. It's a powerful library, and one you will continually return to as you advance in text analysis (and looks great on your CV!).\n",
    "\n",
    "Because scikit-learn is such a large and powerful library the goal today is not to become experts, but instead learn the basic functions in the library and gain an intuition about how you might use it to do text analysis. We'll give you the keys to the kingdom: you go explore! To give an overview, here are some of the things you can do using scikit-learn:\n",
    "* word weighting\n",
    "* feature extraction\n",
    "* text classification / supervised machine learning\n",
    "    * L2 regression\n",
    "    * classification algorithms such as nearest neighbors, SVM, and random forest\n",
    "* clustering / unsupervised machine learning\n",
    "    * k-means\n",
    "    * pca\n",
    "    * cosine similarity\n",
    "    * LDA\n",
    "\n",
    "Today, we'll start with the Document Term Matrix (DTM). The DTM is the bread and butter of most computational text analysis techniques, both simple and more sophisticated methods. In this lesson we will use Python's scikit-learn package learn to make a document term matrix from a .csv Music Reviews dataset (collected from MetaCritic.com). We will then use the DTM and a word weighting technique called tf-idf (term frequency inverse document frequency) to identify important and discriminating words within this dataset (utilizing the Pandas package). The illustrating question: what words distinguish reviews of Rap albums, Indie Rock albums, and Jazz albums? \n",
    "\n",
    "Finally, we will use the DTM to get an introduction to one method for uncovering patterns or themes within text: LDA, a topic modeling algorithm. Again, this will just be an introduction. Look for additional workshops in the future that will get into topic modeling in more detail.\n",
    "  \n",
    "\n",
    "### Learning Goals\n",
    "* Understand the DTM and why it's important to text analysis\n",
    "* Learn how to create a DTM from a .csv file\n",
    "* Learn basic functionality of Python's package scikit-learn\n",
    "* Understand tf-idf scores, and word scores in general\n",
    "* Learn a simple way to identify distinctive words\n",
    "* Implement a basic topic modeling algorithm and learn how to tweak it\n",
    "* In the process, gain more familiarity and comfort with the Pandas package and manipulating data\n",
    "\n",
    "### Outline\n",
    "<ol start=\"0\">\n",
    "  <li>The Pandas Dataframe: Music Reviews</li>\n",
    "  <li>Explore the Data using Pandas</li>\n",
    "          -Basic descriptive statistics\n",
    "  <li>Creating the DTM: scikit-learn</li>\n",
    "          -CountVectorizer function\n",
    "  <li>What can we do with a DTM?</li>\n",
    "  <li>Tf-idf scores</li>\n",
    "          -TfidfVectorizer function\n",
    "  <li>Identifying Distinctive Words</li>\n",
    "          -Application: Identify distinctive words by genre\n",
    "  <li>Uncovering patterns using LDA</li>\n",
    "</ol>\n",
    "\n",
    "### Key Jargon\n",
    "* *Document Term Matrix*:\n",
    "    * a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms.\n",
    "* *TF-IDF Scores*: \n",
    "    *  short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "* *Topic Modeling*:\n",
    "    * A statistical model to uncover abstract topics within a text. It uses the co-occurrence fo words within documents, compared to their distribution across documents, to uncover these abstract themes. The output is a list of weighted words, which indicate the subject of each topic, and a weight distribution across topics for each document.\n",
    "    \n",
    "* *LDA*:\n",
    "    * Latent Dirichlet Allocation. A implementation of topic modeling that assumes a Dirichlet prior. It does not take document order into account, unlike other topic modeling algorithms.\n",
    "    \n",
    "### Further Resources\n",
    "\n",
    "[This blog post](https://de.dariah.eu/tatom/feature_selection.html) goes through finding distinctive words using Python in more detail \n",
    "\n",
    "Paper: [Fightin’ Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf), Burt Monroe, Michael Colaresi, Kevin Quinn\n",
    "\n",
    "[More detailed description of implementing LDA using scikit-learn](http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-topics-extraction-with-nmf-lda-py).\n",
    "    \n",
    "### 0. The Pandas Dataframe: Music Reviews\n",
    "\n",
    "First, we read our music reviews corpus, which is stored as a .csv file on our hard drive, into a Pandas dataframe. \n",
    "\n",
    "Note: I love Pandas for data munging and basic calculations because it's so easy to use, and its data structure is really intuitive for me. It's not memory efficient however, so you might quickly need to move away from it. I recommend always always always using Pandas (or similar) over spreadsheets and Excel. [Excel is bad for science!](https://www.washingtonpost.com/news/wonk/wp/2016/08/26/an-alarming-number-of-scientific-papers-contain-excel-errors/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "#create a dataframe called \"df\"\n",
    "df = pandas.read_csv(\"../A-Data/BDHSI2016_music_reviews.csv\", sep = '\\t', encoding = 'utf-8')\n",
    "\n",
    "#view the dataframe\n",
    "#notice the metadata. The column \"body\" contains our text of interest.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print the first review from the column 'body'\n",
    "df['body'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explore the Data using Pandas\n",
    "\n",
    "Let's first look at some descriptive statistics about this dataset, to get a feel for what's in it. We'll do this using the Pandas package. \n",
    "\n",
    "Note: this is always good practice. It serves two purposes. It checks to make sure your data is correct, and there's no major errors. It also keeps you in touch with your data, which will help with interpretation. <3 your data!\n",
    "\n",
    "First, what genres are in this dataset, and how many reviews in each genre?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We can count this using the value_counts() function\n",
    "df['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing most people do is to describe their data. (This is the summary command in R, or the sum command in Stata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#There's only one numeric column in our data so we only get one column for output.\n",
    "#If there were multiple numeric columns we would get more columns.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who are the reviewers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['critic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the artists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['artist'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we just want the average score given?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df['score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly more complicted to code: what is the average score for each genre? To do this, we use Pandas *groupby* function. Note: If you are planning on doing any sort of statistics, including basic statistics, you'll want to get very familiar with the groupby function. It's quite powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a groupby dataframe grouped by genre\n",
    "df_genres = df.groupby(\"genre\")\n",
    "print(df_genres)\n",
    "#calculate the mean score by genre, print out the results\n",
    "print(df_genres['score'].mean().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating the DTM: scikit-learn\n",
    "\n",
    "Ok, that's the summary of the metadata. Next, we turn to analyzing the text of the reviews. Remember, the text is stored in the 'body' column. First, a preprocessing step to remove numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['body'] = df['body'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to turn the text into a document term matrix using the scikit-learn function called CountVectorizer. There are two ways to do this. We can turn it into a sparse matrix type, which can be used within scikit-learn for further analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import the function CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvec = CountVectorizer()\n",
    "\n",
    "sklearn_dtm = CountVectorizer().fit_transform(df.body)\n",
    "print(sklearn_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is called Compressed Sparse Format. It save a lot of memory to store the dtm in this format, but it is difficult to look at for a human. To illustrate the techniques in this lesson we will first convert this matrix back to a Pandas dataframe, a format we're more familiar with. For larger datasets, you will have to use the Compressed Sparse Format. Putting it into a DataFrame, however, will enable us to get more comfortable with Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we do the same as we did above, but covert it into a Pandas dataframe. Note this takes quite a bit more memory, so will not be good for bigger data.\n",
    "dtm_df = pandas.DataFrame(countvec.fit_transform(df.body).toarray(), columns=countvec.get_feature_names(), index = df.index)\n",
    "\n",
    "#view the dtm dataframe\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What can we do with a DTM?\n",
    "\n",
    "We can do a number of calculations using a DTM. For a toy example, we can quickly identify the most frequent words (compare this to how many steps it took in lesson 2, where we found the most frequent words using NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dtm_df.sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####Exercise:\n",
    "###Print out the most infrequent words rather than the most frequent words.\n",
    "##Gold star challenge: print the average number of times each word is used in a review\n",
    "#Print this out sorted from highest to lowest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What else does the DTM enable? Because it is in the format of a matrix, we can perform any matrix algebra or vector manipulation on it, which enables some pretty exciting things (think vector space and Euclidean  geometry). But, what do we lose when we reprsent text in this format?\n",
    "\n",
    "Today, we will use variations on the DTM to find distinctive words in this dataset, and then do some preliminary work discovering themes in text.\n",
    "\n",
    "### 4. Tf-idf scores\n",
    "\n",
    "How to find distinctive words in a corpus is a long-standing question in text analysis. We saw a few ways to this yesterday, using natural language processing. Today, we'll learn one simple approach to this: word scores. The idea behind words scores is to weight words not just by their frequency, but by their frequency in one document compared to their distribution across all documents. Words that are frequent, but are also used in every single document, will not be distinguising. We want to identify words that are unevenly distributed across the corpus.\n",
    "\n",
    "One of the most popular ways to weight words (beyond frequency counts) is *tf-idf* scores. By offsetting the frequency of a word by its document frequency (the number of documents in which it appears) will in theory filter out common terms such as 'the', 'of', and 'and'.\n",
    "\n",
    "More precisely, the inverse document frequency is calculated as such:\n",
    "\n",
    "number_of_documents / number_documents_with_term\n",
    "\n",
    "so:\n",
    "\n",
    "tfidf_word1 = word1_frequency_document1 * (number_of_documents / number_document_with_word1)\n",
    "\n",
    "You can, and often should, normalize the numerator: \n",
    "\n",
    "tfidf_word1 = (word1_frequency_document1 / word_count_document1) * (number_of_documents / number_document_with_word1)\n",
    "\n",
    "We can calculate this manually, but scikit-learn has a built-in function to do so. We'll use it, but a challenge for you: use Pandas to calculate this manually. \n",
    "\n",
    "To do so, we simply do the same thing we did above with CountVectorizer, but instead we use the function TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import the function\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfvec = TfidfVectorizer()\n",
    "\n",
    "#create the dtm, but with cells weigthed by the tf-idf score.\n",
    "dtm_tfidf_df = pandas.DataFrame(tfidfvec.fit_transform(df.body).toarray(), columns=tfidfvec.get_feature_names(), index = df.index)\n",
    "\n",
    "#view results\n",
    "dtm_tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the 20 words with highest tf-idf weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dtm_tfidf_df.max().sort_values(ascending=False)[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! We have successfully identified content words, without removing stop words. What else do you notice about this list?\n",
    "\n",
    "### 5. Identifying Distinctive Words\n",
    "\n",
    "What can we do with this? These scores are best used when you want to identify distinctive words for individual documents, or groups of documents, compared to other groups or the corpus as a whole. To illustrate this, let's compare three genres and identify the most distinctive words by genre.\n",
    "\n",
    "First we merge the genre of the document into our dtm weighted by tf-idf scores, and then compare genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creat dataset with document index and genre\n",
    "df_genre = df['genre'].to_frame()\n",
    "print(df_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#merge this into the dtm_tfidf_df\n",
    "merged_df = df_genre.join(dtm_tfidf_df, how = 'right', lsuffix='_x')\n",
    "\n",
    "#view result\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare the words with the highest tf-idf weight for each genre. \n",
    "\n",
    "Note: there are other ways to do this. Challenge: what is a different approach to identifying rows from a certain genre in our dtm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pull out the reviews for three genres, Rap, Alternative/Indie Rock, and Jazz\n",
    "dtm_rap = merged_df[merged_df['genre_x']==\"Rap\"]\n",
    "dtm_indie = merged_df[merged_df['genre_x']==\"Alternative/Indie Rock\"]\n",
    "dtm_jazz = merged_df[merged_df['genre_x']==\"Jazz\"]\n",
    "\n",
    "#print the words with the highest tf-idf scores for each genre\n",
    "print(\"Rap Words\")\n",
    "print(dtm_rap.max(numeric_only=True).sort_values(ascending=False)[0:20])\n",
    "print()\n",
    "print(\"Indie Words\")\n",
    "print(dtm_indie.max(numeric_only=True).sort_values(ascending=False)[0:20])\n",
    "print()\n",
    "print(\"Jazz Words\")\n",
    "print(dtm_jazz.max(numeric_only=True).sort_values(ascending=False)[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! A method of identifying distinctive words. You notice there are some proper nouns in there. How might we remove those if we're not interested in them?\n",
    "\n",
    "Tf-idf scores are just one way to identify distinctive or discriminating words. See Monroe, Colaresi, and Quinn (2009) for more ideas for finding distinctive words. (Warning: this paper is a bit outdated. No one has taken up their recommendation to use a Dirichlet prior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####Exercise:\n",
    "###Copy and paste the code above to this cell, and change the genres for a different comparison.\n",
    "###Instead of outputting the highest weighted words, output the lowest weighted words. \n",
    "##How should we interpret these words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Uncovering Patterns: LDA\n",
    "\n",
    "Frequency counts and tf-idf scores are done at the word level. There are other methods of exporatory or unsupervised analysis on the document level and by examining the co-occurrence of words within documents. Scikit-learn allows for many of these methods, including:\n",
    "\n",
    "* document clustering\n",
    "* document or word similarities using cosine similarity\n",
    "* pca\n",
    "* topic modeling\n",
    "\n",
    "We'll run through an example of topic modeling here. Again, the goal is not to learn everything you need to know about topic modeling. Instead, this will provide you some starter code to run a simple model, with the idea that you can use this base of knowledge to explore this further.\n",
    "\n",
    "We will run Latent Dirichlet Allocation, the most basic and the oldest version of topic modeling. We will run this in one big chunk of code. Our challenge: use our knowledge of scikit-learn that we gained aboe to walk through the code to understand what it is doing. Your challenge: figure out how to modify this code to work on your own data, and/or tweak the parameters to get better output.\n",
    "\n",
    "Note: we will be using a different dataset for this technique. The music reviews in the above dataset are often short, one word or one sentence reviews. Topic modeling is not really appropriate for texts that are this short. Instead, we want texts that are longer and are composed of multiple topics each. For this exercise we will use a database of children's literature from the 19th century. \n",
    "\n",
    "The data were compiled by students in this course: http://english197s2015.pbworks.com/w/page/93127947/FrontPage\n",
    "Found here: http://dhresourcesforprojectbuilding.pbworks.com/w/page/69244469/Data%20Collections%20and%20Datasets#demo-corpora\n",
    "\n",
    "That page has additional corpora, for those interested in exploring text analysis further.\n",
    "\n",
    "I did some minimal cleaning to get the children's literature data in .csv format for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_lit = pandas.read_csv(\"../A-Data/childrens_lit.csv.bz2\", sep='\\t', encoding = 'utf-8', compression = 'bz2')\n",
    "\n",
    "#drop rows where the text is missing. I think there's only one row where it's missing, but check me on that.\n",
    "df_lit = df_lit.dropna(subset=['text'])\n",
    "\n",
    "#view the dataframe\n",
    "df_lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to fit the model. This requires the use of CountVecorizer, which we've already used, and the scikit-learn function LatentDirichletAllocation.\n",
    "\n",
    "See [here](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) for more information about this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####Adopted From: \n",
    "#Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_samples = 2000\n",
    "n_topics = 5\n",
    "n_top_words = 50\n",
    "\n",
    "##This is a function to print out the top words for each topic in a pretty way.\n",
    "#Don't worry too much about understanding every line of this code.\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "# Use tf-idf features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.80, min_df=50,\n",
    "                                   max_features=None,\n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(df_lit.text)\n",
    "\n",
    "# Use tf (raw term count) features\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.80, min_df=50,\n",
    "                                max_features=None,\n",
    "                                stop_words='english'\n",
    "                                )\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(df_lit.text)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_topics=%d...\"\n",
    "      % (n_samples, n_topics))\n",
    "\n",
    "#define the lda function, with desired options\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=20,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=80.,\n",
    "                                total_samples=n_samples,\n",
    "                                random_state=0)\n",
    "#fit the model\n",
    "lda.fit(tf)\n",
    "\n",
    "#print the top words per topic, using the function defined above.\n",
    "#Unlike R, which has a built-in function to print top words, we have to write our own for scikit-learn\n",
    "#I think this demonstrates the different aims of the two packages: R is for social scientists, Python for computer scientists\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####Exercise:\n",
    "###Run the same code as above but change some of the parameters. How does this change the output.\n",
    "###Suggestions:\n",
    "## 0. Use tf-idf scores rather than raw counts. (hint: look for the variable name we created) \n",
    "## 1. Change the number of topics. What do you find?\n",
    "## 2. Do not remove stop words. How does this change the output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we may want to do with the output is find the most representative texts for each topic. A simple way to do this (but not memory efficient), is to merge the topic distribution back into the Pandas dataframe.\n",
    "\n",
    "First get the topic distribution array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_dist = lda.transform(tf)\n",
    "topic_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge back in with the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_dist_df = pandas.DataFrame(topic_dist)\n",
    "df_w_topics = topic_dist_df.join(df_lit)\n",
    "df_w_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can sort the dataframe for the topic of interest, and view the top documents for the topics.\n",
    "Below we sort the documents first by Topic 0 (looking at the top words for this topic I think it's about family, health, and domestic activities), and next by Topic 1 (again looking at the top words I think this topic is about children playing outside in nature). These topics may be a family/nature split?\n",
    "\n",
    "Look at the titles for the two different topics. Look at the gender of the author. Hypotheses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df_w_topics[['title', 'author gender', 0]].sort_values(by=[0], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df_w_topics[['title', 'author gender', 1]].sort_values(by=[1], ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What other patterns might we find with topic modeling? Toward what end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Ex (gold star exercise!): \n",
    "#       Find the most prevalent topic in the corpus.\n",
    "#       Find the least prevalent topic in the corpus. \n",
    "#       Find the most prevalent topic by the gender of the author.\n",
    "#       Hint: How do we define prevalence? What are different ways of measuring this,\n",
    "#              and the benefits/drawbacks of each.\n",
    "\n",
    "\n",
    "#       Extra bonus gold star exercise:\n",
    "#          This topic model provide the topic distribtution for 127 rows, but there are 131 rows in the full data.\n",
    "#          What is going on here? (I don't have an answer to this. I hope someone can figure it out!)           "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
