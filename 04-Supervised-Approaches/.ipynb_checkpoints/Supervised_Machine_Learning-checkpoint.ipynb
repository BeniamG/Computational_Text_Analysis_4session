{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Supervised Machine Learning\n",
    "\n",
    "This two hour workshop covers text classification.  The goal of text classification is to classify texts into any number of predefined categories. This method is most similar to traditional content analysis, or text coding, in that it does the same thing as a team of trained coders: place texts into categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification does require labeled text, or text that is already categorized into predefined categories. In some cases, like ours today, this is built into the data. Labeling text can also be done by hand, as was the case in the reading for today. Once we have a good number of labeled texts, usually between 200 and 500, we can use superivsed machine learning algorithms to train a computer to recognize the categories and place the remaining, un-coded texts into a category. This method has two benefits: (1) It allows us to scale our hand-coding up almost indefinitely, and (2) it identifies what *features* (in our case, words), are most defining of each category. This can help us learn more about the content of our categories.\n",
    "\n",
    "We will apply supervised machine methods to a corpus distributed by Ted Underwood and Jordan Sellers in support of their own literary historical study on nineteenth- and early-twentieth century volumes of poetry that were reviewed in prestigious magazines versus not at all. (The idea being that even a negative review indicates valuable, critical engagement.)\n",
    "\n",
    "In essence, our task will be to learn the vocabulary of literary prestige, building on the Canon/Archive paper (https://litlab.stanford.edu/LiteraryLabPamphlet11.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get you comfortable with the basic vocabulary around supervised machine learning and text classification\n",
    "* Understand the intuition behind supervised machine learning\n",
    "* Learn how to implement a few key supervised machine learning algorithms\n",
    "* Understand how to test for accuracy\n",
    "* Use scikit-learn to identify important features for each category\n",
    "* Be equipped with foundational knowledge so you can continue to learn, either on your own or by taking more advanced math, machine learning, or text analysis courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Reading in and pre-processing data](#1.-Reading-in-and-pre-processing-data)\n",
    "* [Divide data into training and test sets](#2.-Divide-data-into-training-and-test-sets)\n",
    "* [Supervised Machine Learning Classification](#3.-Supervised-Machine-Learning-Classification)\n",
    "    1. [Exercise 3.1](#Exercise-3.1)\n",
    "* [Cross Validation](#4.-Cross-Validation)\n",
    "* [Prediction](#5.-Prediction)\n",
    "    1. [Exercise 5.1](#Exercise-5.1)\n",
    "* [Identifying Features](#6.-Identifying-Features)\n",
    "    1. [Exercise 6.1](#Exercise-6.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *supervised machine learning* :\n",
    "    * a machine learning task of inferring a classification from labeled training data.\n",
    "* *features*:\n",
    "    * way of representing the object that will be classified. For images, features are often pixels. For text, features are usually word counts or weighted word counts, but they also also be things like a word's part of speech.\n",
    "* *training set*:\n",
    "    * a selection of labeled data that is used to train the machine learning algorithm\n",
    "* *test set*:\n",
    "    * a selection of labeled data that is used to test the accuracy of the machine learning algorithm\n",
    "* *unseen set*:\n",
    "    * a selection of *unlabeled* data - the machine learning algorithm predicts the label for these data\n",
    "* *accuracy*:\n",
    "    * the number of texts the algorithms correctly classifies\n",
    "* *cross validation*:\n",
    "    * a way to assess how the algorithm will perform on an unseen data set\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [full documentation](http://scikit-learn.org/stable/supervised_learning.html) for supervised machine learning using scikit-learn\n",
    "\n",
    "[Identifying what types of blog posts are censored in China](http://gking.harvard.edu/publications/how-censorship-china-allows-government-criticism-silences-collective-expression), using supervised machine learning, Gary King, Jennifer Pan, and Margaret E Roberts\n",
    "\n",
    "\n",
    "[Literary Pattern Recognition](https://lucian.uchicago.edu/blogs/literarynetworks/files/2015/12/LONG_SO_CI.pdf), Hoyt Long, Richard So\n",
    "\n",
    "[How Quickly Do Literary Standards Change?](https://tedunderwood.com/2015/05/18/how-quickly-do-literary-standards-change/), Ted Underwood, Jordan Sellers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading in and pre-processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll need to make sure we know what version of Pyton and sklearn we are using.  The very first number is the version of python you are using.  This notbook requires version 3.x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "\n",
    "print(\"The Python version is %s.%s.%s.\" % sys.version_info[:3])\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the code above, you'll need to choose which version of `cross_val_score` you need to install.  Depending on your version of scikit-learn, you'll need to choose the appropirate line to un-comment (remove the hashtag from the start of the line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#first, import the necessary modules\n",
    "import pandas\n",
    "import numpy as np\n",
    "#scikit-learn is a huge libaray. We import what we need.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC #Linear Suppot Vector Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB #Naive Bayes classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier #nearest neighbors classifier\n",
    "from sklearn.metrics import accuracy_score #to asses the accuracy of the algorithm\n",
    "\n",
    "###for scikit-learn version 0.17.x, choose the first line; for 0.18.x, choose the second line:\n",
    "#from sklearn.cross_validation import cross_val_score #to compute cross validation for assessment purposes - v 0.17.x\n",
    "from sklearn.model_selection import cross_val_score #to compute cross validation for assessment purposes-v 0.18.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will read the texts and turn them into lists.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read our texts and turn them into lists\n",
    "import os\n",
    "review_path = '../A-Data/poems/reviewed/'\n",
    "random_path = '../A-Data/poems/random/'\n",
    "review_files = os.listdir(review_path)\n",
    "random_files = os.listdir(random_path)\n",
    "\n",
    "review_texts = [open(review_path+file_name).read() for file_name in review_files]\n",
    "random_texts = [open(random_path+file_name).read() for file_name in random_files]\n",
    "\n",
    "review_texts[0] #notice the strange output here. These poems are saved in a bag of words format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the strange output here? These poems are saved in a bag of words format.\n",
    "\n",
    "Next we'll create a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#transform and concat these lists into a Pandas dataframe\n",
    "df1 = pandas.DataFrame(review_texts, columns = ['body'])\n",
    "df1['label'] = \"review\"\n",
    "df2 = pandas.DataFrame(random_texts, columns = ['body'])\n",
    "df2['label'] = \"random\"\n",
    "df = pandas.concat([df1,df2])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Divide data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to create a training set and a test set. We'll train on the first 500 poems, and test the accuracy on the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#randomize our rows\n",
    "df = df.sample(720, random_state=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create two new dataframes\n",
    "df_train = df[:500]\n",
    "df_test = df[500:]\n",
    "print(df_test['label'].value_counts())\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Supervised Machine Learning Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create a dtm for each review, and an array containing the classification label for each review (for us, this is called 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#transform the 'body' column into a document term matrix\n",
    "tfidfvec = TfidfVectorizer(stop_words = 'english', min_df = 1, binary=True)\n",
    "countvec = CountVectorizer(stop_words = 'english', min_df = 1, binary=True)\n",
    "\n",
    "#remember, vocabularis might be different in the two sets. This code accomdidates that fact.\n",
    "training_dtm_tf = countvec.fit_transform(df_train.body)\n",
    "test_dtm_tf = countvec.transform(df_test.body)\n",
    "\n",
    "#create an array for labels\n",
    "training_labels = df_train.label\n",
    "test_labels = df_test.label\n",
    "test_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define a container for our chosen algorithm, in this case multinomial naive bayes\n",
    "nb = MultinomialNB()\n",
    "\n",
    "#fit a model on our training set\n",
    "nb.fit(training_dtm_tf, training_labels)\n",
    "\n",
    "#predict the labels on the test set using the trained model\n",
    "predictions_nb = nb.predict(test_dtm_tf) \n",
    "predictions_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Is it possible to create a datafame that includes the labels and estimations next to one another? Yes!\n",
    "list(zip(test_labels, predictions_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the built-in function `accuracy-score` to calculate the accuracy of our classifier. For binary and multiclass classification, which is our case, this function calculates Jaccard similarity coefficient, defined as the size of the intersection divided by the size of the union of two label sets. It is used to compare the set of predicted labels (the labels the algorithms assigned to the test set) to the true labels for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score(predictions_nb, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's see if tf-idf weigting improves the accuracy\n",
    "\n",
    "training_dtm_tfidf = tfidfvec.fit_transform(df_train.body)\n",
    "test_dtm_tfidf = tfidfvec.transform(df_test.body)\n",
    "nb.fit(training_dtm_tfidf, training_labels)\n",
    "predictions_tfidf = nb.predict(test_dtm_tfidf) \n",
    "accuracy_score(predictions_tfidf, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's try a different classifier, LinearSVC\n",
    "svc = LinearSVC()\n",
    "svc.fit(training_dtm_tf, training_labels)\n",
    "predictions_svc = svc.predict(test_dtm_tf) \n",
    "accuracy_score(predictions_svc, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#k nearest neighbor\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(training_dtm_tf, training_labels)\n",
    "predictions_knn = knn.predict(test_dtm_tf) \n",
    "accuracy_score(predictions_knn, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Try to improve the accuracy by changing some of the options.  You can change options for either the vectorizer function, or one of the algorithms. How does it change the accuracy of the classifier? (Lowering the accuracy can tell you something too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the accuracy for the test set we created, but what if we have a bunch of poems that are not yet classified and we want to classify them? Can we be sure we'll get a similar accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##We need to define a new container for CountVectorizer. Note this, and we'll come back to it below\n",
    "countvec_cv = CountVectorizer(stop_words = 'english', min_df = 1, binary=True)\n",
    "dtm = countvec_cv.fit_transform(df.body)\n",
    "scores = cross_val_score(nb, dtm, df.label, cv=5)\n",
    "scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: This assumes any unseen data has similar features as your training and test data. If the features are different, you can not assume you will get similar accuracy. When might the unseen data be different from the labeled data?\n",
    "\n",
    "Question: Why is the cross validation accuracy different than our classifier? \n",
    "\n",
    "Turn to the person next to you and talk through the above code. What is happening in each line?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often what we want to do next is predict the label for unlabeled texts. Let's predict the label for two poems where we do not know the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dickinson_canonic = \"\"\"Because I could not stop for Death – \n",
    "He kindly stopped for me –  \n",
    "The Carriage held but just Ourselves –  \n",
    "And Immortality.\n",
    "\n",
    "We slowly drove – He knew no haste\n",
    "And I had put away\n",
    "My labor and my leisure too,\n",
    "For His Civility – \n",
    "\n",
    "We passed the School, where Children strove\n",
    "At Recess – in the Ring –  \n",
    "We passed the Fields of Gazing Grain –  \n",
    "We passed the Setting Sun – \n",
    "\n",
    "Or rather – He passed us – \n",
    "The Dews drew quivering and chill – \n",
    "For only Gossamer, my Gown – \n",
    "My Tippet – only Tulle – \n",
    "\n",
    "We paused before a House that seemed\n",
    "A Swelling of the Ground – \n",
    "The Roof was scarcely visible – \n",
    "The Cornice – in the Ground – \n",
    "\n",
    "Since then – ‘tis Centuries – and yet\n",
    "Feels shorter than the Day\n",
    "I first surmised the Horses’ Heads \n",
    "Were toward Eternity – \"\"\"\n",
    "\n",
    "\n",
    "anthem_patriotic = \"\"\"O! say can you see, by the dawn's early light,\n",
    "What so proudly we hailed at the twilight's last gleaming,\n",
    "Whose broad stripes and bright stars through the perilous fight,\n",
    "O'er the ramparts we watched, were so gallantly streaming?\n",
    "And the rockets' red glare, the bombs bursting in air,\n",
    "Gave proof through the night that our flag was still there;\n",
    "O! say does that star-spangled banner yet wave\n",
    "O'er the land of the free and the home of the brave?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform these into DTMs with the same feature-columns as previously.  Notice which CountVectorizer container we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unknown_dtm = countvec.transform([dickinson_canonic,anthem_patriotic]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Why do we use this one? Why did we need to define a new one for the cross validation step above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb.predict(unknown_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either return a binary classification, but we know that Bayes theorem assigns a probability of membership in either category\n",
    "\n",
    "Just how confident is our classifier of its predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb.predict_proba(unknown_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's zip this together with the name of the poems to make sense of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(zip(['dickinson', 'anthem'], nb.predict(unknown_dtm), nb.predict_proba(unknown_dtm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and process the 'canonic' (albeit unreviewed) volumes of poetry.  Use the poetry classifier to predict whether they might have been reviewed.  Does the output make sense? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "canonic_path = '../A-Datadata/poems/canonic/'\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Identifying Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use supervised machine learning to classify unseen documents using the above code. But we can also use it to learn more about the content of each category, by extracting the most defining features of this category. So even if we do not have unseen text, we can use this method to better understand given categories (for example, canonized and non-canonized text). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hand-waving the underlying statistics here...\n",
    "\n",
    "def most_informative_features(text_class, vectorizer = countvec, classifier = nb, top_n = 20):\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    class_index = np.where(classifier.classes_==(text_class))[0][0]\n",
    "    \n",
    "    class_prob_distro = np.exp(classifier.feature_log_prob_[class_index])\n",
    "    alt_class_prob_distro = np.exp(classifier.feature_log_prob_[1 - class_index])\n",
    "    \n",
    "    odds_ratios = class_prob_distro / alt_class_prob_distro\n",
    "    odds_with_fns = sorted(zip(odds_ratios, feature_names), reverse = True)\n",
    "    \n",
    "    return odds_with_fns[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns feature name and odds ratio for a given class\n",
    "most_informative_features('review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_informative_features('random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What kinds of patterns do you notice among the 'most informative features'?  Try looking at the top fifty most informative words for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
